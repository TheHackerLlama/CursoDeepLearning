{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Deep Learning\n",
    "\n",
    "Este es el taller introductorio a cómo funciona Deep Learning. Si no sabes Python, no te preocupes, es muy fácil de utilizar. Esto es un cuaderno de Jupyter. Los cuadernos de Jupyter son muy útiles ya que permiten tener celdas de texto (como esta) y celdas de Python. Todo se mantiene en un ambiente, por lo cuál las variables se mantienen mientras tengas el cuaderno abierto. Esto quiere decir que podemos utilizar variables de una celda en otra celda. Este cuaderno sólo está para ayudarte a empezar con las herramientas que vamos a utilizar. Al final hay una sección con las soluciones a los problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda importamos dos librerías que vamos a utilizar: pandas y numpy.\n",
    "    \n",
    "* [Pandas](https://pandas.pydata.org/): Estructuras de datos fáciles de usar para información más compleja. Aquí sólo vamos a utilizar [DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html), una estructura muy similar a una tabla. \n",
    "* [NumPy](http://www.numpy.org/): Esta librería la vamos a usar extensivamente a través del curso. NumPy tiene herramientas para computación científica muy potentes. Por ejemplo, utilizaremos numpy.array, los cuales son mucho más compactos. Esto es muy importante cuando empecemos a utilizar cientos de miles de ejemplos. [Una explicación un poco más detallada](https://stackoverflow.com/questions/993984/why-numpy-instead-of-python-lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hola qué tal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si eres completamente nuevo en Python, no te preocupes. Python tiene una sintaxis muy sencilla y concisa, por lo que es muy fácil de aprender mientras vayas desarrollando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Hola mundo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  Hola mundo\n"
     ]
    }
   ],
   "source": [
    "print(\"test: \", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Intro a Python\n",
    "Empezaremos con una introducción de Python. Esta será breve porque Python es sencillo y hay muchos recursos donde aprenderlo en muy poco tiempo. Si te sientes cómodo con Python, salta a la sección del Perceptrón.\n",
    "\n",
    "Esto es una variable. Como ven, no se debe asignar el tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El output de una celda de Python es la última expresión del programa. \n",
    "\n",
    "También podemos asignar varias variables a la vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 10, 5\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto es un comentario\n",
    "if a > b:\n",
    "    print(\"A es más grande\")\n",
    "else:\n",
    "    print(\"B es más grande\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [2, 4, 6, 8, 10, 12, 14]\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí accedemos a los valores de la lista de diferentes maneras.\n",
    "print(lst[0])\n",
    "print(lst[-1])\n",
    "print(lst[1:3])# Esto es un slice. Te permite acceder a varios elementos a la vez\n",
    "print(lst[1:5])\n",
    "print(lst[2:]) # ¿Podrías explicar qué está ocurriendo aquí?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lst:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frutas = [\"manzana\", \"peras\", \"platanos\"]\n",
    "for i, item in enumerate(frutas):\n",
    "    print(i, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suma(nums):\n",
    "    \"\"\"\n",
    "    Este es un comentario de muchas líneas. Es buena práctica utilizarlos para \n",
    "    describir funciones y al inicio de cada archivo.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for n in nums:\n",
    "        total = total + n\n",
    "    return total\n",
    "suma(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay *muchas* cosas de Python que no hemos visitado. Si quieren aprender más, hay muchos recursos. Aquí hemos visitado los temas básicos (variables, condicionales, listas, iteraciones, funciones), pero hay muchos temas como clases, funciones lambda y mucho más. \n",
    "\n",
    "Con estas herramientas deberían ser capaces de empezar a desarrollar los siguientes problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un perceptrón nos ayuda a tomar decisiones. Por el momento hemos visto que se puede hacer para ver si ir a un festival de quesos o aceptar alumnos, pero lo podemos hacer para todo tipo de cosas. Podemos representar operaciones como NOT, AND, NAND y OR con un perceptrón.\n",
    "\n",
    "Haremos, primero, un NOT de ejemplo. El NOT tiene un input nada más. Si el input es 0, NOT 0 es verdadero. Si es 1, NOT 1 es falso. Por lo tanto, debemos ajustar dos parámetros: el peso y el bias. Queremos que cuando hagamos la operación input * weight + bias (esta es la fórmula del perceptrón, sólo que ahora tenemos un elemento), si el resultado es mayor o igual a 0, se active la neurona (es decir, devuelva verdadero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [0, 1]\n",
    "\n",
    "weight = -1\n",
    "bias = 0.5\n",
    "\n",
    "for i in test_inputs:\n",
    "    linear_combination = i*weight + bias\n",
    "    output = linear_combination >= 0\n",
    "    print(\"Combinacion linear: \", linear_combination)\n",
    "    print(\"Output: \", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección vamos a implementar un perceptrón básico. Este perceptrón funcionará como una compuerta AND. No te desmotives si no lograr encontrar la solución, al final están las soluciones. \n",
    "\n",
    "A diferencia del NOT, para la compuerta AND tenemos dos entradas, por lo tanto, tenemos dos pesos que asignar y un bias. En la siguiente celda ajusta los pesos y bias. Puedes correr la otra celda para ver los resultados en la tabla de verdad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Asignar los pesos y el bias para una compuerta AND\n",
    "weight1 = 3\n",
    "weight2 = 1\n",
    "bias = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuviste 3 errores.  ¡Sigue intentando!\n",
      "\n",
      "Input 1    Input 2    Combinacion Lineal    Activation Output   Es correcto\n",
      "      0          0                     1                    1            No\n",
      "      0          1                     2                    1            No\n",
      "      1          0                     4                    1            No\n",
      "      1          1                     5                    1            Si\n"
     ]
    }
   ],
   "source": [
    "# No cambies esta celda, pero te sugiero revisarla\n",
    "\n",
    "# Estos son los cuatro casos de la tabla de verdad que vamos a probar.\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "# Vamos a ir generando el output y viendo si la respuesta está bien\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    # Aquí sumamos la multiplicación de las entradas con sus pesos, más el bias.\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    \n",
    "    # Si la combinación lineal es positiva, el perceptrón se encenderá\n",
    "    output = int(linear_combination >= 0)\n",
    "    \n",
    "    # Revisamos si la respuesta está bien\n",
    "    is_correct_string = 'Si' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Combinacion Lineal', '  Activation Output', '  Es correcto'])\n",
    "if not num_wrong:\n",
    "    print('¡Muy bien!  Tuviste todas bien.\\n')\n",
    "else:\n",
    "    print('Tuviste {} errores.  ¡Sigue intentando!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de Activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos antes, el perceptrón usa como función de activación una función escalón. Aunque esta está bien para explicar cómo funciona la neurona, normalmente queremos que nuestro input pueda tener diferentes valores que no vayan de 0 a 1.\n",
    "\n",
    "La función sigmoidal da como resultados números que van de 0 a 1. Recuerda, la entrada de esta función es la combinación lineal de la neurona. Esto quiere decir que primero multiplicamos los valores por sus pesos y a eso le sumamos el bias. Esta es la entrada de la neurona, y la neurona se activa según la función. En el caso de la sigmoidal, es la siguiente:\n",
    "\n",
    "![Funcion Sigmoidal](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/image_01_068.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    TODO: Implement sigmoid function\n",
    "    Use the method np.exp().\n",
    "    \n",
    "    Arguments:\n",
    "    x: A value\n",
    "    \n",
    "    Return:\n",
    "    s: The sigmoid of x\n",
    "    \"\"\" \n",
    "    return \n",
    "\n",
    "# No modifiques esto. Aquí estamos creando dos arreglos utilizando la librería NumPy. Esto lo usaremos de prueba\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "# TODO: Calcula la combinación linear. Recuerda que puedes utilizar el método .dot en un np.array. \n",
    "# Recuerda, la combinación lineal es el producto punto de la entrada y sus pesos, y a eso se le suma el bias.\n",
    "linear_combination = \n",
    "\n",
    "# TODO: Activa la neurona utilizando la función sigmoidal que acabas de implementar\n",
    "output = \n",
    "\n",
    "## No modifiques lo siguiente\n",
    "print('Output:', output)\n",
    "\n",
    "if int(output*10000) == 4329:\n",
    "    print(\"El output está bien\")\n",
    "else:\n",
    "    print(\"El output está mal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque por el momento hemos utilizado sigmoid() para recibir números, este también funciona perfectamente bien cuando recibe un arreglo. Al recibir un arreglo, como estamos utilizando np.exp(), sigue funcionando. Esto nos permite calcular multiples activaciones a la vez y nos vendrá bien para construir redes neuronales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente ejercicio implementaremos la parte llamada Feedforward de una red neuronal.\n",
    "\n",
    "En este proceso, la red neuronal recibe una entrada, y propaga sus valores multiplicándolos por sus pesos, sumando el bias, y activando las neurona. En este ejercicio implementaremos una red neuronal (técnicamente no, pero es lo que más se asemeja) para el ejercicio de los aplicantes de universidad. \n",
    "![title](nn_2_1_student_notation1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este es un ejemplo de entrada y pesos de la capa input a la capa output\n",
    "input = np.array([8.5, 9.5])\n",
    "weights_input_output = np.array([0.33, 0.25])\n",
    "\n",
    "print(input)\n",
    "print(weights_input_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí hacemos la combinacion linear\n",
    "bias = -4.5\n",
    "linear_combination = input.dot(weights_input_output) + bias\n",
    "linear_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activamos la neurona con la función sigmoid\n",
    "result = sigmoid(linear_combination)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hacer una función que haga el feedforward para cualquier input\n",
    "def feed_forward(input):\n",
    "    \"\"\" Función que hace el cálculo \n",
    "        TODO: implementa esta función.\n",
    "        Debe hacer el proceso anterior de multiplicar por los pesos, sumar el bias y activar.\n",
    "        Usa los valores de arriba para los pesos y el bias\n",
    "    \"\"\"\n",
    "    weights_input_output = \n",
    "    bias = \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos utilizar este valor para clasificar si aceptamos o no al estudiante. \n",
    "# Si es mayor a 0.5, aceptamos al estudiante. Esto es una sobresimplificación.\n",
    "print(feed_forward(np.array([8.5, 9.5])))\n",
    "print(feed_forward(np.array([5.5, 6.5])))\n",
    "print(feed_forward(np.array([4.5, 9.5])))\n",
    "print(feed_forward(np.array([9.5, 9.5])))\n",
    "print(feed_forward(np.array([5.5, 9.5])))\n",
    "print(feed_forward(np.array([8.5, 8.5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejercicio anterior hicimos una red de dos capas. Una de entrada y una de salida. En Deep Learning, trabajamos con capas sobre capas (red profunda). La lógica es la misma, pero un poco más compleja. Implementaremos la siguiente red:\n",
    "![title](nn_2_3_1_student.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.array([8.5, 9.5])\n",
    "weights_input_hidden = np.array([[0.12, 0.04, 0.08], [0.2, 0.03, 0.05]])\n",
    "\n",
    "print(input)\n",
    "print(weights_input_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = -2\n",
    "hidden = sigmoid(input.dot(weights_input_hidden) + bias)\n",
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_hidden_output = np.array([1.2, 0.2, 0.3])\n",
    "bias = -0.85\n",
    "result = sigmoid(hidden.dot(weights_hidden_output) + bias)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hacer una función que haga el feedforward para cualquier input\n",
    "def feed_forward(input):\n",
    "    \"\"\" Función que hace el cálculo \"\"\"\n",
    "    weights_input_hidden = np.array([[0.12, 0.04, 0.08], [0.2, 0.03, 0.05]])\n",
    "    bias_input_hidden = -2\n",
    "    \n",
    "    weights_hidden_output = np.array([1.2, 0.2, 0.3])\n",
    "    bias_hidden_output = -0.85\n",
    "    \n",
    "    # TODO: Implementa el valor de las neuronas en la capa oculta y en la capa de salida\n",
    "    hidden =\n",
    "    result = \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feed_forward(np.array([8.5, 9.5])))\n",
    "print(feed_forward(np.array([5.5, 6.5])))\n",
    "print(feed_forward(np.array([4.5, 9.5])))\n",
    "print(feed_forward(np.array([9.5, 9.5])))\n",
    "print(feed_forward(np.array([5.5, 9.5])))\n",
    "print(feed_forward(np.array([8.5, 8.5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte introduciremos a una nueva funcion de activación. La función ReLU (Rectifier . Relu es muy sencilla y tiene propiedades que han demonstrado ser excelentes en Deep Learning. La red neuronal es la misma que antes. No veras beneficios directamente ahora, pero es bueno que conozcan una segunda función.\n",
    "\n",
    "![Funcion ReLU](https://qph.ec.quoracdn.net/main-qimg-4229dd280e03b7b3a5dc26c808c4b15b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    TODO: Implement relu function\n",
    "    Use the method np.maximum().\n",
    "    \n",
    "    Arguments:\n",
    "    x: A value\n",
    "    \n",
    "    Return:\n",
    "    s: The relu of x\n",
    "    \"\"\" \n",
    "    return \n",
    "\n",
    "def feed_forward(input):\n",
    "    \"\"\" Función que hace el cálculo \"\"\"\n",
    "    weights_input_output = np.array([[0.12, 0.2], [0.04, 0.03], [0.08, 0.05]]).transpose()\n",
    "    bias_input_hidden = -2\n",
    "    \n",
    "    weights_hidden_output = np.array([[1.2, 0.2, 0.3]]).transpose()\n",
    "    bias_hidden_output = -0.85\n",
    "    \n",
    "    hidden = relu(input.dot(weights_input_hidden) + bias_input_hidden)\n",
    "    result = sigmoid(hidden.dot(weights_hidden_output) + bias_hidden_output)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feed_forward(np.array([8.5, 9.5])))\n",
    "print(feed_forward(np.array([5.5, 6.5])))\n",
    "print(feed_forward(np.array([4.5, 9.5])))\n",
    "print(feed_forward(np.array([9.5, 9.5])))\n",
    "print(feed_forward(np.array([5.5, 9.5])))\n",
    "print(feed_forward(np.array([8.5, 8.5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soluciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Asignar los pesos y el bias para un AND\n",
    "weight1 = 0.6\n",
    "weight2 = 0.6\n",
    "bias = -1\n",
    "\n",
    "# TODO: Implement sigmoid function\n",
    "def sigmoid(x):\n",
    "    # TODO: Implement sigmoid function\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# TODO: Calcula la activación linear.\n",
    "linear_combination = inputs.dot(weights) + bias\n",
    "\n",
    "# TODO: Activa la neurona utilizando la función sigmoidal que acabas de implementar\n",
    "output = sigmoid(linear_combination)\n",
    "            \n",
    "# TODO: Hacer una función que haga el feedforward para cualquier input\n",
    "def feed_forward(input):\n",
    "    \"\"\" Función que hace el cálculo \n",
    "        TODO: implementa esta función.\n",
    "        Debe hacer el proceso anterior de multiplicar por los pesos, sumar el bias y activar.\n",
    "    \"\"\"\n",
    "    weights_input_output = np.array([0.33, 0.25])\n",
    "    bias = -4.5\n",
    "    return sigmoid(input.dot(weights_input_output) + bias)\n",
    "\n",
    "# TODO: Hacer una función que haga el feedforward para cualquier input\n",
    "def feed_forward(input):\n",
    "    \"\"\" Función que hace el cálculo \"\"\"\n",
    "    weights_input_hidden = np.array([[0.12, 0.04, 0.08], [0.2, 0.03, 0.05]])\n",
    "    bias_input_hidden = -2\n",
    "    \n",
    "    weights_hidden_output = np.array([1.2, 0.2, 0.3])\n",
    "    bias_hidden_output = -0.85\n",
    "    \n",
    "    # TODO: Implementa el valor de las neuronas en la capa oculta y en la capa de salida\n",
    "    hidden = sigmoid(input.dot(weights_input_hidden) + bias_input_hidden)\n",
    "    result = sigmoid(hidden.dot(weights_hidden_output) + bias_hidden_output)\n",
    "    \n",
    "    return result\n",
    "                 \n",
    "# TODO: Implementa la función ReLU\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    TODO: Implement relu function\n",
    "    Use the method np.maximum().\n",
    "    \n",
    "    Arguments:\n",
    "    x: A value\n",
    "    \n",
    "    Return:\n",
    "    s: The relu of x\n",
    "    \"\"\" \n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
