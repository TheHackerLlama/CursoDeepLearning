{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio vamos a implementar Gradient Descent para regresión linear. Aunque en este ejercicio sólo vamos a ir ajustando un peso y un bias para encontrar la mejor línea (modelo) que represente la información, es importante entender que este mismo algoritmo es el que va ajustando todos los pesos en una red neuronal para minimizar el error.\n",
    "\n",
    "Vamos a utilizar la información de un archivo csv (comma separated values). En la siguiente celda importamos la información, y las separamos en trainX y trainY. En trainX, tenemos los features. En este caso sólo tenemos uno (asumamos que es el tamaño de la casa). En trainY tenemos el valor que querremos predecir, o sea el costo de la casa. \n",
    "\n",
    "En la siguiente celda graficamos estos puntos utilizando matplotlib en un scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Leemos la info de un archivo csv\n",
    "with open('data.csv', 'r') as f:\n",
    "  r = csv.reader(f)\n",
    "  points = list(r)\n",
    "\n",
    "# Separamos la información en trainX y trainY\n",
    "# En trainX tenemos nuestros features. \n",
    "# En este ejemplo sólo tenemos un feature, metros cuadrados\n",
    "# En trainY tenemos la variable que queremos predecir, en este caso\n",
    "# el valor de la casa en miles de dólares.\n",
    "trainX = []\n",
    "trainY = []\n",
    "for point in points:\n",
    "    trainX.append(float(point[0])) \n",
    "    trainY.append(float(point[1]))\n",
    "plt.scatter(trainX, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora implementaremos el algoritmo de Gradient Descent.\n",
    "\n",
    "Para comenzar, vamos a hacer una función que recibiendo el tamaño de la casa, el weight y el bias, podemos calcular el precio. Esto utiliza una ecuación de una línea recta (y = mx + b), pero para red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implementa la función h(x) = w*x + b Función que predice costo de casa\n",
    "def predict(size, weight, bias):\n",
    "    \"\"\"\n",
    "    Función que predice el costo de una casa con una línea.\n",
    "    \n",
    "    Utiliza la función h(x) = x * w + b\n",
    "    \n",
    "    size: El tamaño de la casa en metros cuadrados\n",
    "    weight: El peso asignado al tamaño de la casa\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    \"\"\"\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda deberás implementar la función de pérdida de Mean Squared Error\n",
    "![title](mse.png)\n",
    "Recuerda utilizar la función predict (h) para hacer más sencillo el algoritmo. Adicionalmente, implementa las derivadas parciales respecto al peso y respecto al bias. **Omite el dos**. Dado que para la función de MSE agregamos una división entre dos, al derivar, se cancelará con el dos del exponente.\n",
    "![title](mse_deriv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implementa la función de pérdida y sus derivadas\n",
    "\n",
    "def loss_function(sizes, costs, weight, bias):\n",
    "    \"\"\"\n",
    "    Función que calcula el error de una propuesta de regresión lineal\n",
    "    \n",
    "    Utiliza la función h(x) = x * w + b\n",
    "    \n",
    "    sizes: Los tamaños de las casas en metros cuadrados.\n",
    "    costs: Los costos verdaderos de las casas en cientos de miles de dólares.\n",
    "    weight: El peso asignado al tamaño de la casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    \"\"\"\n",
    "    totalError = 0\n",
    "    m = len(sizes)\n",
    "    \n",
    "    # Calcula el error con diferencia de cuadrados\n",
    "    for i in range(0, m):\n",
    "        # ¿Cuál es el valor de entrada y cuál es el valor verdadero?\n",
    "        x = \n",
    "        y = \n",
    "        \n",
    "        # ¿Qué valor predicen tus datos? Utiliza la función predict\n",
    "        predicted = \n",
    "        \n",
    "        # Revisa la función para calcular el error\n",
    "        totalError += \n",
    "        \n",
    "    # Revisa la fórmula de MSE (Mean Squared Error)\n",
    "    return \n",
    "\n",
    "\n",
    "def loss_function_deriv_weight(size, cost, weight, bias, n):\n",
    "    \"\"\"\n",
    "    Función que calcula la derivada parcial según el peso de MSE\n",
    "    \n",
    "    Utiliza la función h'(x) = (-1/n) * (y - (mx + b))\n",
    "    \n",
    "    size: El tamaño de una casa en metros cuadrados.\n",
    "    cost: El costo real de la casa.\n",
    "    weight: El peso asignado al tamaño de esa casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    n: El tamaño del dataset.\n",
    "    \"\"\"\n",
    "    return \n",
    "\n",
    "def loss_function_deriv_bias(size, cost, weight, bias, n):\n",
    "    \"\"\"\n",
    "    Función que calcula la derivada parcial según el bias de MSE\n",
    "    \n",
    "    Utiliza la función h'(x) = -1/n * (y - (mx + b))\n",
    "    \n",
    "    size: El tamaño de una casa en metros cuadrados.\n",
    "    cost: El costo real de la casa.\n",
    "    weight: El peso asignado al tamaño de esa casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    n: El tamaño del dataset.\n",
    "    \"\"\"\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente función implementaremos un algoritmo que ajusta el peso y el bias ligeramente minimizando el error. Para lograr esto, calculamos la derivada de la función de pérdida. Acumulamos estos valores y al final ajustamos ligeramente el peso y el bias. El ajuste se realiza de la siguiente manera\n",
    "\n",
    "![title](gradient_step.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula el paso de gradient\n",
    "def step_gradient(sizes, costs, weight, bias, learning_rate):\n",
    "    \"\"\"\n",
    "    Función que propone un nuevo bias y peso que reduce la función de error.\n",
    "    \n",
    "    sizes: Los tamaños de la casa.\n",
    "    cost: Los costos reales de la casa.\n",
    "    weight: Los pesos de la casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    learning_rate: Qué tan rápido va a aprender\n",
    "    \"\"\"\n",
    "    \n",
    "    # El valor acumulado de la derivada en diferentes puntos. Inicialmente es 0.\n",
    "    weight_deriv = 0\n",
    "    bias_deriv = 0\n",
    "    \n",
    "    # El tamaño del dataset\n",
    "    m = float(len(sizes))\n",
    "    \n",
    "    # Para cada casa\n",
    "    for i in range(len(sizes)):\n",
    "        # ¿Cuál es su valor en X y en Y?\n",
    "        x = \n",
    "        y = \n",
    "        \n",
    "        # Calcula las derivadas parciales del loss function\n",
    "        bias_deriv += \n",
    "        weight_deriv += \n",
    "    \n",
    "    # Da un paso en la dirección que minimice el error\n",
    "    weight = weight - \n",
    "    bias = bias - \n",
    "\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, vamos a crear una función que se encarga de entrenar el modelo utilizando Gradient Descent. Para cada epoch, vamos a realizar un step_gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena nuestro modelo con Gradient Descent\n",
    "def train(sizes, costs, initial_weight, initial_bias, learning_rate, epoch):\n",
    "    \"\"\"\n",
    "    Función que utiliza Gradient Descent para minimizar error.\n",
    "    \n",
    "    sizes: Los tamaños de la casa.\n",
    "    cost: Los costos reales de la casa.\n",
    "    intial_weight: El peso inicial del modelo.\n",
    "    initial_bias: El bias inicial del modelo.\n",
    "    learning_rate: Qué tan rápido va a aprender.\n",
    "    epoch: Número de iteraciones sobre dataset.\n",
    "    \"\"\"\n",
    "    weight = initial_weight\n",
    "    bias = initial_bias\n",
    "   \n",
    "    # Itera varias veces sobre el dataset para minimizar el error\n",
    "    for i in range(epoch):\n",
    "        # Llama a step_gradient con sus argumentos correctos.\n",
    "        weight, bias = \n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print(\"Loss: \", str(loss_function(sizes, costs, weight, bias)), \"b: \" , bias, \"w: \", weight)\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda definimos el peso y bias inicial. Adicionalmente, definimos el learning_rate y el número de epochs. Corre la celda para entrenar el modelo. La pérdida debería ser cercana a 56. Para comprobar resultados, revisa las siguientes celdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "weight = 0.0\n",
    "bias = 0.0\n",
    "epochs = 100\n",
    "\n",
    "weight, bias = train(trainX, trainY, bias, weight, learning_rate, epochs)\n",
    "print(bias, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que el modelo está entrenado (hemos definido un bias y un weight que describen el dataset), podemos predecir el valor de y para nuevos valores de x. En la siguiente celda utilizamos predict con el weight y bias definidos, y lo graficamos para ver si tiene sentido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_value = 35\n",
    "predicted = predict(new_value, weight, bias)\n",
    "plt.scatter(trainX, trainY)\n",
    "plt.scatter(new_value, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda haremos lo mismo pero para todos los puntos de 30 a 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(weight, bias):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(30, 70):\n",
    "        predicted = predict(i, weight, bias)\n",
    "        x.append(i)\n",
    "        y.append(predicted)\n",
    "    plt.scatter(trainX, trainY)\n",
    "    plt.scatter(x, y)\n",
    "    \n",
    "plot(weight, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección vamos a implementar una red neuronal con Feed Forward y Backpropagation sin utilizar librerías. No tendrás que programar nada, enfócate en entender el comportamiento de esta red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
      " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
      "[[-0.5910955 ]\n",
      " [ 0.75623487]\n",
      " [-0.94522481]\n",
      " [ 0.34093502]]\n"
     ]
    }
   ],
   "source": [
    "# Se inicializa el peso con promedio de 0\n",
    "weights0 = 2 * np.random.random((3,4)) - 1\n",
    "weights1 = 2 * np.random.random((4,1)) - 1\n",
    "\n",
    "print(weights0)\n",
    "print(weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.496410031903\n",
      "Error: 0.00858452565325\n",
      "Error: 0.00578945986251\n",
      "Error: 0.00462917677677\n",
      "Error: 0.00395876528027\n",
      "Error: 0.00351012256786\n",
      "Error: 0.00318350238587\n",
      "Error: 0.00293230634228\n",
      "Error: 0.00273150641821\n",
      "Error: 0.00256631724004\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for i in range(100000):\n",
    "    # Feed forward\n",
    "    l0 = X\n",
    "    l1 = sigmoid(np.dot(l0, weights0))\n",
    "    l2 = sigmoid(np.dot(l1, weights1))\n",
    "    \n",
    "    l2_error = y - l2\n",
    "    l2_delta = l2_error*sigmoid_deriv(l2)\n",
    "    \n",
    "    # Agregamos a una lista para luego crear visualización\n",
    "    errors.append(np.mean(np.abs(l2_error)))\n",
    "    if (i% 10000) == 0:\n",
    "        print(\"Error:\", str(np.mean(np.abs(l2_error))))\n",
    "    \n",
    "    \n",
    "    l1_error = l2_delta.dot(weights1.T)\n",
    "    l1_delta = l1_error * sigmoid_deriv(l1)\n",
    "    \n",
    "    weights1 += l1.T.dot(l2_delta)\n",
    "    weights0 += l0.T.dot(l1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x111db8a90>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1053f0860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(len(errors)), errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soluciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implementa la función h(x) = w*x + b Función que predice costo de casa\n",
    "def predict(size, weight, bias):\n",
    "    \"\"\"\n",
    "    Función que predice el costo de una casa con una línea.\n",
    "    \n",
    "    Utiliza la función h(x) = x * w + b\n",
    "    \n",
    "    size: El tamaño de la casa en metros cuadrados\n",
    "    weight: El peso asignado al tamaño de la casa\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    \"\"\"\n",
    "    return size * weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implementa la función de pérdida y sus derivadas\n",
    "\n",
    "def loss_function(sizes, costs, weight, bias):\n",
    "    \"\"\"\n",
    "    Función que calcula el error de una propuesta de regresión lineal\n",
    "    \n",
    "    Utiliza la función h(x) = x * w + b\n",
    "    \n",
    "    sizes: Los tamaños de las casas en metros cuadrados.\n",
    "    costs: Los costos verdaderos de las casas en cientos de miles de dólares.\n",
    "    weight: El peso asignado al tamaño de la casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    \"\"\"\n",
    "    totalError = 0\n",
    "    m = len(sizes)\n",
    "    \n",
    "    # Calcula el error con diferencia de cuadrados\n",
    "    for i in range(0, m):\n",
    "        # ¿Cuál es el valor de entrada y cuál es el valor verdadero?\n",
    "        x = sizes[i] \n",
    "        y = costs[i] \n",
    "        \n",
    "        # ¿Qué valor predicen tus datos? Utiliza la función predict\n",
    "        predicted = predict(x, weight, bias) \n",
    "        \n",
    "        # Revisa la función para calcular el error\n",
    "        totalError += (y - predicted) ** 2\n",
    "        \n",
    "    # Revisa la fórmula de MSE (Mean Squared Error)\n",
    "    return totalError / (2 * float(m))\n",
    "\n",
    "\n",
    "def loss_function_deriv_weight(size, cost, weight, bias, n):\n",
    "    \"\"\"\n",
    "    Función que calcula la derivada parcial según el peso de MSE\n",
    "    \n",
    "    Utiliza la función h'(x) = (-1/n) * (y - (mx + b))\n",
    "    \n",
    "    size: El tamaño de una casa en metros cuadrados.\n",
    "    cost: El costo real de la casa.\n",
    "    weight: El peso asignado al tamaño de esa casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    n: El tamaño del dataset.\n",
    "    \"\"\"\n",
    "    return -(1/n) * size * (cost - predict(size, weight, bias))\n",
    "\n",
    "def loss_function_deriv_bias(size, cost, weight, bias, n):\n",
    "    \"\"\"\n",
    "    Función que calcula la derivada parcial según el bias de MSE\n",
    "    \n",
    "    Utiliza la función h'(x) = -1/n * (y - (mx + b))\n",
    "    \n",
    "    size: El tamaño de una casa en metros cuadrados.\n",
    "    cost: El costo real de la casa.\n",
    "    weight: El peso asignado al tamaño de esa casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    n: El tamaño del dataset.\n",
    "    \"\"\"\n",
    "    return -(1/n) * (cost - predict(size, weight, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula el paso de gradient\n",
    "def step_gradient(sizes, costs, weight, bias, learning_rate):\n",
    "    \"\"\"\n",
    "    Función que propone un nuevo bias y peso que reduce la función de error.\n",
    "    \n",
    "    sizes: Los tamaños de la casa.\n",
    "    cost: Los costos reales de la casa.\n",
    "    weight: Los pesos de la casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    learning_rate: Qué tan rápido va a aprender\n",
    "    \"\"\"\n",
    "    \n",
    "    # El valor acumulado de la derivada en diferentes puntos. Inicialmente es 0.\n",
    "    weight_deriv = 0\n",
    "    bias_deriv = 0\n",
    "    \n",
    "    # El tamaño del dataset\n",
    "    m = float(len(sizes))\n",
    "    \n",
    "    # Para cada casa\n",
    "    for i in range(len(sizes)):\n",
    "        # ¿Cuál es su valor en X y en Y?\n",
    "        x = sizes[i]\n",
    "        y = costs[i]\n",
    "        \n",
    "        # Calcula las derivadas parciales del loss function\n",
    "        bias_deriv += loss_function_deriv_bias(x, y, weight, bias, m)\n",
    "        weight_deriv += loss_function_deriv_weight(x, y, weight, bias, m)\n",
    "    \n",
    "    # Da un paso en la dirección que minimice el error\n",
    "    weight = weight - (weight_deriv * learning_rate)\n",
    "    bias = bias - (learning_rate * bias_deriv)\n",
    "\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena nuestro modelo con Gradient Descent\n",
    "def train(sizes, costs, initial_weight, initial_bias, learning_rate, epoch):\n",
    "    \"\"\"\n",
    "    Función que utiliza Gradient Descent para minimizar error.\n",
    "    \n",
    "    sizes: Los tamaños de la casa.\n",
    "    cost: Los costos reales de la casa.\n",
    "    intial_weight: El peso inicial del modelo.\n",
    "    initial_bias: El bias inicial del modelo.\n",
    "    learning_rate: Qué tan rápido va a aprender.\n",
    "    epoch: Número de iteraciones sobre dataset.\n",
    "    \"\"\"\n",
    "    weight = initial_weight\n",
    "    bias = initial_bias\n",
    "   \n",
    "    # Itera varias veces sobre el dataset para minimizar el error\n",
    "    for i in range(epoch):\n",
    "        # Llama a step_gradient con sus argumentos correctos.\n",
    "        weight, bias = step_gradient(sizes, costs, weight, bias, learning_rate)\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print(\"Loss: \", str(loss_function(sizes, costs, weight, bias)), \"b: \" , bias, \"w: \", weight)\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
