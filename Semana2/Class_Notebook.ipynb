{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# conda install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x14035c978>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13b076be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainX = [100, 200, 300, 400, 500, 600, 700, 800, 1000]\n",
    "trainY = [350, 380, 400, 450, 550, 600, 650, 700, 750]\n",
    "plt.scatter(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que predice costo de casa\n",
    "def predict(size, weight, bias):\n",
    "    # y = mx + b\n",
    "    return size * weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que calcula el error\n",
    "def loss_function(sizes, costs, weight, bias):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(sizes)):\n",
    "        x = sizes[i]\n",
    "        y = costs[i]\n",
    "        totalError += (y - (weight * x + bias)) ** 2\n",
    "    return totalError / float(len(sizes))\n",
    "\n",
    "def loss_function_deriv_weight(size, cost, weight, bias, n):\n",
    "    # -1/n * (y - (mx + b))\n",
    "    return -(1/n) * size * (cost - predict(size, weight, bias))\n",
    "\n",
    "def loss_function_deriv_bias(size, cost, weight, bias, n):\n",
    "    # -1/n * (y - (mx + b))\n",
    "    return -(1/n) * (cost - predict(size, weight, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(sizes, costs, weight, bias,learningRate):\n",
    "    weight_deriv = 0\n",
    "    bias_deriv = 0\n",
    "    m = float(len(sizes))\n",
    "    \n",
    "    for i in range(len(sizes)):\n",
    "        x = sizes[i]\n",
    "        y = costs[i]\n",
    "        \n",
    "        # Calculate partial derivatives of loss_function\n",
    "        bias_deriv += -(2/m) * (y - ((weight * x) + bias))\n",
    "        weight_deriv += -(2/m) * x * (y - ((weight * x) + bias))\n",
    "    \n",
    "    weight = weight - (weight_deriv * learning_rate)\n",
    "    bias = bias - (learning_rate * bias_deriv)\n",
    "\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sizes, costs, initial_weight, initial_bias, learning_rate, epoch):\n",
    "    weight = initial_weight\n",
    "    bias = initial_bias\n",
    "   \n",
    "    for i in range(epoch):\n",
    "        weight, bias = step_gradient(sizes, costs, weight, bias, learning_rate)\n",
    "        if i%10 == 0:\n",
    "            print(\"Loss: \", str(loss_function(sizes, costs, weight, bias)), \"b: \" , bias, \"w: \", weight)\n",
    "    return [b, m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('data.csv', 'r') as f:\n",
    "  r = csv.reader(f)\n",
    "  points = list(r)\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "for point in points:\n",
    "    trainX.append(float(point[0])) \n",
    "    trainY.append(float(point[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1484.586557408649 b:  0.014547010110737297 w:  0.7370702973591052\n",
      "Loss:  112.65166489759584 b:  0.029714049245227046 w:  1.4781595857319891\n",
      "Loss:  112.64990814400619 b:  0.030329617731073667 w:  1.4788952310786534\n",
      "Loss:  112.64954700524882 b:  0.030930464674392876 w:  1.4788841757704807\n",
      "Loss:  112.6491859224223 b:  0.031531251467959626 w:  1.4788723682497276\n",
      "Loss:  112.64882489409928 b:  0.032131992902294806 w:  1.4788605608606418\n",
      "Loss:  112.64846392027134 b:  0.03273268899573464 w:  1.4788487543619513\n",
      "Loss:  112.64810300093023 b:  0.0333333397517163 w:  1.4788369487543538\n",
      "Loss:  112.64774213606768 b:  0.033933945173661606 w:  1.4788251440377829\n",
      "Loss:  112.64738132567561 b:  0.03453450526499217 w:  1.4788133402121713\n",
      "0.02963934787473239 1.4774173755483797\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "weight = 0.0\n",
    "bias = 0.0\n",
    "epochs = 1000\n",
    "\n",
    "[bias, weight] = train(trainX, trainY, bias, weight, learning_rate, epochs)\n",
    "print(bias, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7370702973591052 0.014547010110737297\n",
      "1484.586557408649\n",
      "1.1067954543515157 0.02187396295959641\n",
      "457.8542575737673\n",
      "1.2922546649131115 0.025579224321293136\n",
      "199.50998572553894\n",
      "1.385283255651245 0.027467789559144355\n",
      "134.50591058200533\n",
      "1.4319472323843205 0.028445071981738963\n",
      "118.14969342239947\n",
      "1.4553540088980408 0.02896524076647862\n",
      "114.0341490603815\n",
      "1.4670946177201354 0.0292561141260467\n",
      "112.99857731713661\n",
      "1.4729832982243762 0.02943196916380713\n",
      "112.7379818756847\n",
      "1.4759365618962286 0.029550129024383073\n",
      "112.67238435909097\n",
      "1.4774173755483797 0.02963934787473239\n",
      "112.65585181499746\n",
      "10 0.02963934787473239 1.4774173755483797 112.65585181499746\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    print(new_m, new_b)\n",
    "    return [new_b, new_m]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "        print(compute_error_for_line_given_points(b, m, points))\n",
    "    return [b, m]\n",
    "\n",
    "\n",
    "points = genfromtxt(\"data.csv\", delimiter=\",\")\n",
    "learning_rate = 0.0001\n",
    "initial_b = 0 # initial y-intercept guess\n",
    "initial_m = 0 # initial slope guess\n",
    "num_iterations = 10\n",
    "[b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "print(num_iterations, b, m, compute_error_for_line_given_points(b, m, points))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
