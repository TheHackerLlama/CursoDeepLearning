{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicación de clasificación de perros\n",
    "\n",
    "En este proyecto implementarás una CNN para clasificar perros utilizando Transfer Learning. Este es el primer proyecto completo que haremos, y, por lo tanto, será más retador. Asegúrate de completar los otros cuadernos antes. Aprenderemos a construir un pipeline que luego puede ser usado una página o aplicación para procesar imágenes reales dadas por un usuario. La idea es que, dada la imagen de un perro, identifique el estimado de que sea una raza. Si la imagen es de un humano, dirá a qué raza de perro se parece.\n",
    "\n",
    "\n",
    "![perro](https://github.com/osanseviero/Dog-Breed-Classifier/raw/master/images/sample_dog_output.png \"Objetivo\")\n",
    "\n",
    "Además de explorar modelos de CNN que son estado del arte para clasificación, también tendrás que tomar decisiones del UX. El objetivo es entender los retos que hay cuando se quieren juntar varios modelos diseñados para diferentes tareas al procesar data. Cada modelo tiene sus fortalezas y sus debilidades, y muchas veces las aplicaciones de la vida real no podrán dar respuestas perfectas. Hay 7 secciones en el proyecto. \n",
    "\n",
    "Antes de comenzar, desde tu ambiente (curso-dl), corre las siguientes líneas\n",
    "\n",
    "* ```conda update --all```\n",
    "* ```conda install scikit-learn```\n",
    "* ```conda install opencv```\n",
    "* ```conda install tqdm```\n",
    "* ```conda install pillow```\n",
    "\n",
    "Después, necesitamos descargar dos datasets.\n",
    "* [Dataset Perro](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip).Después de descomprimirlo, ponlo en la carpeta /dogImages. \n",
    "* [Dataset humanos](http://vis-www.cs.umass.edu/lfw/lfw.tgz). Después de descomprimirlo, ponlo en la carpeta /lfw\n",
    "* Y descarga los [features de VGG16](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG16Data.npz). Ponlo en la carpeta /bottleneck_features.\n",
    "\n",
    "## Paso 0. Importar Datasets\n",
    "\n",
    "### Dataset de perros\n",
    "En la siguiente celda, importamos un dataset con imágenes de perros. Cargamos los archivos con scikit-learn y cargamos:\n",
    "\n",
    "* train_files, valid_files, test_files - arreglos con las direcciones al entrenamiento, validación y testing\n",
    "\n",
    "* train_targets, valid_targets, test_targets - arreglos con los labels (y) ya habiendo hecho el one-hot encoding\n",
    "\n",
    "* dog_names - lista de las razas de perros para poder traducir los labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# función para cargar dataset dado un path\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# cargamos training, testing y validation dataset. Ya vienen separados\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# Cargamos los nombres de los perros según el nombre de las carpetas.\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# Imprimimos algunos datos del dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset de humanos\n",
    "En la siguiente celda, importamos un dataset con imágenes de humanos. Aquí no tenemos training ni testing, dado que no es parte del problema de clasificar perros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1. Detección de humanos\n",
    "\n",
    "Para detectar las caras de personas en las imágenes, utilizaremos algo que ya está implementado en OpenCV llamado Haar feature-based cascade classifier. OpenCV viene con varios detectores de cara pre-entrenados que ya están guardados como XML en GitHub. En el directorio llamado haarcascades puedes encontrar uno de estos detectores. \n",
    "\n",
    "Aquí te muestro un ejemplo de cómo usarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# conseguimos el detector de cara pre-entrenado\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# cargamos una imagen de humano en BGR. Si sólo encuentra una cara, \n",
    "# puedes cambiar este número (se cargan aleatoriamente)\n",
    "img = cv2.imread(human_files[123])\n",
    "\n",
    "# convertimos la imagen BGR a grayscale. No nos importa el color\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# encontramos las caras en la imagen con el método detectMultiScale\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# conseguimos bounding box para cada cara encontrada (sólo tenemos una en este problema)\n",
    "for (x,y,w,h) in faces:\n",
    "    # agregamos un bounding box encima de la imagen original\n",
    "    cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "    \n",
    "# convertimos imagen BGR a RGB para poder graficarla\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Mostramos la imagen\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de utilizar cualquier detector de cara, es buena práctica convertir la imagen a escala de grises. El método ```detectMultiScale``` ejecuta el clasificador en face_cascade y recibe esa imagen en escala de grises como parámetros.\n",
    "\n",
    "faces, el resultado de la función, es un np.array con las caras detectadas, donde cada fila corresponde a una cara. Cada cara es un arreglo con 4 entradas que especifican cómo es el bounding box. Los primeros dos elementos (x y y) son las posiciones de las esquinas de la izquierda. Los otros dos elementos (w y h) son el width y height del bounding box.\n",
    "\n",
    "### Implementando un detector de caras\n",
    "Podemos seguir este proceso para escribir una función que devuelve True si hay una cara de humano y False sino. La función recibe como parámetro el path del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Revisa qué tan bueno es este detector.\n",
    "Cambia la siguiente celda para ver el performance del detector que acabamos de implementar. \n",
    "* ¿Qué porcentaje de las primeras 100 imágenes de humanos tienen una cara detectada?\n",
    "* ¿Qué porcentaje de las primeras 100 imágenes de perros tienen una cara detectada?\n",
    "\n",
    "Idealmente, tendríamos 100% para las imágenes de humanos y 0% para las imágenes de perros. El algoritmo no es perfecto, pero tiene un buen desempeño (aceptable). Extraemos los paths de las primeras 100 imágenes y las guardamos en np.arrays. Responde estas preguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# No modifiques el código de arriba\n",
    "\n",
    "## TODO: Prueba el desempeño del detector con los dos arreglos de arriba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta 2.** Este proceso algoritmico necesita comunicar al usuario que sólo se aceptan imágenes de humanos donde se tenga una clara vista de la cara (sino, los usuarios se podrían frustar). En tu opinión, ¿es una expectativa razonable para imponer en el usuario? ¿Se te ocurre algún método para detectar humanos en imágenes qe no necesiten tener la cara presentada propiamente?\n",
    "\n",
    "\n",
    "## Paso 2. Detección de Perros\n",
    "En esta sección, utilizaremos un modelo pre-entrenado de [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) para detectar perros. La primera línea de código descarga ResNet-50, con pesos que han sido entrenados para [ImageNet](http://www.image-net.org/), el dataset largo y popular de clasificación de imágenes. ImageNet tiene más de 10 millones de URLs, cada uno linkeando a una imagen conteniendo un objeto de una de 1000 categorías. Dada una imagen, este modelo pre-entrenado devuelve su predicción de las categorías de ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# definimos el modelo ResNet50\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamos la información\n",
    "\n",
    "Las redes neuronales convolucionales en Keras requieren un arreglo de 4 dimensiones de input (tensor 4D) como entrada, con un tamaño de (samples, rows, columns, channels). La siguiente función, path_to_tensor, convierte el path de una imagen de color a un tensor 4D que podremos dar a una CNN en Keras. Esta función primero carga la imagen y la convierte en un cuadrado de 224x224. Después, la imagen se convierte en un arreglo que se ajusta a 4D. Como trabajamos con imágenes de color, tenemos 3 canales. Como sólo procesamos una imagen, el tensor siempre tendrá el tamaño (1, 224, 224, 3).\n",
    "\n",
    "En el caso de la función paths_to_tensor, se recibe un arreglo con los paths y se regresa un tensor 4D con la forma (samples, 224, 224, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # carga la imagen RGB\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    \n",
    "    # convierte la imagen a un tensor 3D de (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    \n",
    "    # convierte tensor 3D a 4D con forma (1, 224, 224, 3)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haciendo predicciones con ResNet\n",
    "\n",
    "Ahora que tenemos los tensores en 4D, sólo necesitamos hacer un poco de preprocesamiento adicional.\n",
    "* Convertir RGB a BGR reordenando los canales\n",
    "* Paso de normalización - se resta el pixel promedio (de todas las imágenes de ImageNet) de cada pixel.\n",
    "\n",
    "Estos dos pasos ya están implementados en la función preprocess_input de Keras.\n",
    "\n",
    "Con las imágenes en el formato correcto, podemos darlas ResNet-50 y extraer las predicciones. Esto lo logramos con el método predict, el cuál devuelve un arreglo donde cada entrada es la probabilidad predecida de que una imagen pertenezca a una clase de ImageNet. Utilizamos la función argmax del vector de probabilidades para obtener el número que corresponde al elemento predecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # regresa vector con predicciones de la imagen\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detector de Perros \n",
    "\n",
    "Revisa el [diccionario de ImageNet](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). No hay una categoría de perros como tal, pero las categorías 151 a 268 corresponden a razas de perros, de Chihuahua a Mexican Hairless. Para detectar si es un perro, sólo necesitamos ver que el valor devuelto por la predicción esté en ese rango."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Revisa qué tan bueno es este detector \n",
    "\n",
    "**Pregunta 3.** Al igual que antes, responde estas preguntas\n",
    "* ¿Qué porcentaje de las imágenes de humanos tienen un perro detectado?\n",
    "* ¿Qué porcentaje de las imágenes de perros tienen un perro detectado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Revisa la precisión utilizando human_files_short y dog_files_short"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3. Crea una CNN que clasifique razas de perros (desde 0)\n",
    "\n",
    "Ahora que tenemos funciones para detectar humanos y perros, vamos a trabajar en predecir la raza de las imágenes. En este paso, crearás una CNN que los clasifique. Deberás crear tu propia CNN (no uses transfer learning) y tu objetivo es un test accuracy de por lo menos...1%, aunque puedes llegar hasta 15. Más adelante utilizarás otra técnica para hacerlo mucho mejor.\n",
    "\n",
    "¡No agregues muchas capas! Mientras más parámetros haya, más de tardará el entrenamiento, por lo que es más probable que necesites un GPU. Por suerte, Keras te da un estimado de cuánto tiempo se tarda por cada epoch. Puedes determinar el tiempo de entrenamiento a partir de eso. \n",
    "\n",
    "El reto de clasificar razas de perros es extremadamente difícil. Incluso los humanos tienen dificultad. Por ejemplo, entre Brittany y un Welsh Springer Spaniel\n",
    "\n",
    "![welsh springer spaniel](https://github.com/osanseviero/Dog-Breed-Classifier/raw/04f4caabcdc4fe0f17ab3c94caf1fc07a562f1fc/images/Welsh_springer_spaniel_08203.jpg \"Welsh Springer Spaniel\")\n",
    "![britanny](https://github.com/osanseviero/Dog-Breed-Classifier/raw/04f4caabcdc4fe0f17ab3c94caf1fc07a562f1fc/images/Brittany_02625.jpg \"Brittany\")\n",
    "\n",
    "No sólo pasa que hay razas similares, sino que una raza puede ser muy diferente, como los labradores (pueden ser crema, chocolate o negros). Siendo 133 clases, si predecimos aleatoriamente, tendríamos menos de 1% de precisión.\n",
    "\n",
    "### Pre Procesamos los datos\n",
    "Reescalamos las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implementa arquitectura CNN.\n",
    "Utiliza una arquitectura que tú quieras. Ya importamos algunos módulos, pero siéntete libre de importar lo que necesites. \n",
    "\n",
    "Una arquitectura de sugerencia, si te pierdes. Al utilizar model.summary() al final, debería desplegar el siguiente resultado. (Nota, el tamaño es 224,224, 3)\n",
    "![\"Texto\"](https://github.com/osanseviero/Dog-Breed-Classifier/raw/04f4caabcdc4fe0f17ab3c94caf1fc07a562f1fc/images/sample_cnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# TODO: Define arquitectura\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta 4**. Explica detalladamente la lógica para el orden de las capas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilamos modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Entrena el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: especiifca el número de epochs.\n",
    "\n",
    "epochs = ...\n",
    "\n",
    "### No modifiques el siguiente código\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos el modelo con menor validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacemos testing del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conseguimos el índice del label de cada uno\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4. Ejemplo de Transfer Learning\n",
    "Para reducir el tiempo sin sacrificar la precisión, aquí utilizaremos un CNN con transfer learning.\n",
    "\n",
    "### Obtenemos features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura del modelo\n",
    "El modelo usa un VGG-16 ya entrenado para extraer los features. El último output convolucional es el input de nuestro modelo. Sólo necesitamos un global average pooling layer y una densa, donde el último contiene un nodo para cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos el mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacemos el testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predecimos raza con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 5. Usa TL para predecir raza de perros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora deberás usar transfer learning para identificar raza con un 60% de precisión (puedes llegar mucho más). En el anterior paso, utilizamos Transfer Learning utilizando VGG-16. En esta sección, deberás utilizar alguno de los siguientes.\n",
    "\n",
    "* [VGG 19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz)\n",
    "* [ResNet 50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz)\n",
    "* [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz)\n",
    "* [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz)\n",
    "\n",
    "Descarga el archivo y guárdalo en la misma carpeta de bottleneck features.\n",
    "\n",
    "### TODO Obten los features\n",
    "\n",
    "bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "\n",
    "train_{network} = bottleneck_features['train']\n",
    "\n",
    "valid_{network} = bottleneck_features['valid']\n",
    "\n",
    "test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Obten features de otro CNN pre entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implementa arquitectura del modelo.\n",
    "\n",
    "**Question 5**: Explica los pasos para diseñar la siguiente arquitectura. Investiga los modelo ya entrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Define tu arquitectura\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compila el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Compila el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrena el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Entrena el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Cargamos los pesos del modelo con la menor pérdida de validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "Resnet50_predictions = ...\n",
    "\n",
    "# Precisión en testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usamos función para predecir raza a partir del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dog_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_Resnet50(path_to_tensor(img_path))\n",
    "    \n",
    "    # obtain predicted vector\n",
    "    predicted_vector = Resnet50_model.predict(bottleneck_feature)\n",
    "    \n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 6. Escribe el algoritmo\n",
    "\n",
    "Ya tenemos el modelo. Es hora de usarlo. Escribe un algoritmo que reciba el path de una imagen y determine si es una persona, un perro, o ninguno.\n",
    "\n",
    "* Si se detecta un perro, se dice la raza.\n",
    "* Si se detecta un humano, se dice a qué raza se parece el humano\n",
    "* Si ninguno, muestra un error.\n",
    "\n",
    "Para detectar si es humano o perro, te sugerimos utilizar face_detector y dog_detector de las primeras secciones.\n",
    "\n",
    "Este programa debe desplegar la imagen, decir si es un perro o un humano y decir la raza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Escribe tu algoritmo. \n",
    "def dog_breed_detector(img_path):\n",
    "    \n",
    "\n",
    "dog_breed_detector(train_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 7. Prueba tu algoritmo\n",
    "\n",
    "Sube 6 imágenes tuyas, de amigos o de perros, y ve qué tan bien predice tu modelo. Sube las imágenes a una carpeta llamada sample_pictures/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files = np.array(glob(\"sample_pictures/*\"))\n",
    "print(sample_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in sample_files:\n",
    "    dog_breed_detector(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
