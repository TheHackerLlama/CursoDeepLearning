{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings y Word2Vec\n",
    "\n",
    "En este cuaderno vamos a ver:\n",
    "* Cómo funciona un word embedding y cómo implementarlo en Keras\n",
    "* Cómo aprender un embedding al entrenar la red neuronal\n",
    "\n",
    "\n",
    "## 1. Word Embeddings\n",
    "Word embeddings otorgan una representación densa de los vectores y sus significados relativos. Son una mejora sobre los modelos más sencillos como bolsa de palabras. Los embeddings se aprenden a partir de los datos y se puede reutilizar en los proyectos. \n",
    "\n",
    "Otras representaciones de las palabras se consideran dispersas porque son muy grandes y dada una palabra, se suele representar con un gran vector lleno de zeros. Con los embeddings, las palabras se representan como vectores densos donde un vector representa la proyección de una palabra en un espacio vectorial continuo. La posición de la palabra en el espacio vectorial se aprende del texto y se basa a partir de las palabras que la rodean cuando se utiliza. A esa posición se le llama embedding. Hay diferentes maneras de aprender el embedding como Word2Vec y GloVe.\n",
    " \n",
    "Los embeddings pueden ser aprendidos como parte de un modelo de Deep Learning. Aunque puede tardarse tiempo, hace que los embeddings sean específicos al dataset.\n",
    "\n",
    "\n",
    "## 2. Embeddings en Keras\n",
    "\n",
    "Keras ya tiene una capa Embedding que se puede usar en redes neuronales con texto.\n",
    "https://keras.io/layers/embeddings/#embedding\n",
    "\n",
    "Requiere que la data de entrada está codificada como enteros. Esto quiere decir que cada palabra sea representada por un entero único. Este paso de preprocesamiento se puede hacer con el Tokenizer API que ya viene con Keras. La capa de embedding se inicializa con pesos aleatorios y aprenderá el embedding a partir del dataset.\n",
    "\n",
    "Los usos que tiene esta capa son:\n",
    "* Si se usa sola, se aprende un word embedding que se puede guardar y usar en otros modelos.\n",
    "* Si se usa como parte de un modelo, el embedding se aprenderá a la vez que el modelo aprende.\n",
    "* Se puede usar para cargar un embedding pre-entrenado utilizando transfer learning.\n",
    "\n",
    "Esta capa debe ser la primera capa oculta de la red y tiene 3 argumentos\n",
    "* input_dim: Tamaño del vocabulario que tenemos.\n",
    "* output_dim: Tamaño del espacio vectorial en donde las palabras serán incrustrado (embedded). Este valor se debe probar, puede ser 32, 100 o más grandes.\n",
    "* input_length: Este es el tamaño de la secuencia de entrada. Si todos tus documentos tienen mil palabras, sería 1000.\n",
    "\n",
    "Por ejemplo, la siguiente capa tiene un vocabulario de 200 palabras (representadas desde 0 a 199), un espacio vectorial de 32 dimensiones y cada documento tiene 50 palabras.\n",
    "\n",
    "```e = Embedding(200, 32, input_length=50)```\n",
    "\n",
    "El output de una capa de embedding es un vector con un embedding para cada palabra del documento de entrada. Si lo quieres usar en una red neuronal, debes aplanarlo con una capa Flatten y conectarlo con una red densa.\n",
    "\n",
    "## 3. Embedding en la práctica\n",
    "\n",
    "Ahora veremos cómo utilizar esto en la práctica para clasificar palabras. Para mantenerlo sencillo, utilizaremos 10 documentos de dos palabras cada uno. Cada documento es clasificado como positivo o como negativo, por lo que es una tarea sencilla de análisis de sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos nuestros documentos y sus respectivos labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente es convertir los documentos a enteros. Keras proporciona la función ```one_hot``` que crea un hash para cada palabra. Estimaremos un vocabulario de 50 para evitar la probabilidad de colisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las secuencias tienen diferentes tamaños y Keras utiliza todas las entradas del mismo tamaño. Vamos a aumentar todas las secuencias de entrada para que tengan un tamaño de 4. Keras ya tiene una función para esto, ```pad_sequences```.\n",
    "\n",
    "https://keras.io/preprocessing/sequence/#pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estamos listos para definir y entrenar nuestro embedding como parte de la red neuronal. Utilizaremos un espacio vectorial de 8 dimensiones. El modelo es un simple ejemplo de clasificación binaria. Es importante notar que el output de la capa de embedding son 4 vectores de 8 dimensiones cada uno. Esto lo aplanamos a un vector de 32 elementos para que pase por la capa densa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Crea una capa secuencial\n",
    "\n",
    "## Todo: Crea una capa embedding con 8 dimensiones. Su input_length debe ser max_length\n",
    "\n",
    "## TODO: Agrega una Flatten Layer\n",
    "\n",
    "\n",
    "## TODO: Agrega una capa densa\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comila con adam y binary_crossentropy. La métrica debe ser accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(padded_docs, labels, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos guardar la capa embedding en un archivo y utilizarlo en otros modelos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Crea una capa secuencial\n",
    "model = Sequential()\n",
    "\n",
    "## Todo: Crea una capa embedding con 8 dimensiones. Su input_length debe ser max_length\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "\n",
    "## TODO: Agrega una Flatten Layer\n",
    "model.add(Flatten())\n",
    "\n",
    "## TODO: Agrega una capa densa\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# TODO: Comila con adam y binary_crossentropy. La métrica debe ser accuracy\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
