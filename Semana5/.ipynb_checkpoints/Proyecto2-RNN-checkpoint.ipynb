{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Este es nuestro segundo proyecto. En este proyecto, gran parte del código ya está implementado. No debes modificar el código donde no es necesario. Recuerda leer las instrucciones. \n",
    "\n",
    "Este proyecto tiene dos partes\n",
    "* Predicción de series de tiempo\n",
    "* Generación de secuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1. Predicción de series de tiempo\n",
    "\n",
    "El objetivo es utilizar una RNN para crear un regresor. En particular, nuestro objetivo será predecir las acciones de Apple con siete días de antelación. El objetivo de esta parte es aprender a utilizar un RNN con Keras. Utilizaremos LSTMs, las cuales previenen muchos problemas de las RNN comunes.\n",
    "\n",
    "### 1.1 Cargar información y visualización\n",
    "\n",
    "Nuestro primer paso es cargar información histórica. Iniciaremos cargando la historia de 140 días del valor de las acciones de Apple. Iniciaremos haciendo algunos pasos de preprocesamiento para preparar la información al modelo de RNN. \n",
    "\n",
    "El primer paso, y buena práctica, es normalizar la información. Esto previene problemas comunes con las funciones de activación cuyas transformaciones pueden dar números muy grandes. Adicionalmente, facilita al modelo el cálculo de las derivadas. En este problema normalizaremos para que todos los valores estén entre 0 y 1, inclusive, pero ten en cuenta que hay otras maneras de normalizar. El archivo apple_normalized ya hace ese paso.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Normalization_(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load in necessary libraries for data input and normalization\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### load in and normalize the dataset\n",
    "dataset = np.loadtxt('data/apple_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veremos cómo se comporta el valor en la serie\n",
    "plt.plot(dataset)\n",
    "plt.xlabel('time period')\n",
    "plt.ylabel('normalized series value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2  Separar nuestra serie de tiempo en secuencias\n",
    "\n",
    "Una serie de tiempo es una secuencia de números que se puede representar así:\n",
    "\n",
    "$$s_{0},s_{1},s_{2},...,s_{P}$$\n",
    "\n",
    "donde $s_{p}$ es el valor numérico de la serie de tiempo en el periodo $p$ y donde $P$ es la longitud de la serie. Para poder utilizar una RNN, debemos tratar el problema de predicción de series de tiempo como un problema de regresión. Para esto, necesitamos tener una ventana que se deslice por la serie para construir un conjunto de pares de entrada y salida. Este proceso se ve en el siguiente gif:\n",
    "\n",
    "![imagen](https://github.com/udacity/aind2-rnn/blob/master/images/timeseries_windowing_training.gif?raw=true)\n",
    "\n",
    "Por ejemplo, utilizando un window de 5, produciríamos un conjunto de pares como en la siguiente tabla:\n",
    "\n",
    "$$\\begin{array}{c|c}\n",
    "\\text{Entrada} & \\text{Salida}\\\\\n",
    "\\hline \\color{CornflowerBlue} {\\langle s_{1},s_{2},s_{3},s_{4},s_{5}\\rangle} & \\color{Goldenrod}{ s_{6}} \\\\\n",
    "\\ \\color{CornflowerBlue} {\\langle s_{2},s_{3},s_{4},s_{5},s_{6} \\rangle } & \\color{Goldenrod} {s_{7} } \\\\\n",
    "\\color{CornflowerBlue}  {\\vdots} & \\color{Goldenrod} {\\vdots}\\\\\n",
    "\\color{CornflowerBlue} { \\langle s_{P-5},s_{P-4},s_{P-3},s_{P-2},s_{P-1} \\rangle } & \\color{Goldenrod} {s_{P}}\n",
    "\\end{array}$$\n",
    "\n",
    "Recuerda que el vector de entrada es de tamaño 4 y que el de salida es un escalar. También, fíjate cómo dada una serie de tiempo de tamaño P y con un tamaño de ventana T = 5, creamos P - 5 pares. Generalizando, podemos crear P - T pares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Implement the function called window_transform_series in my_answers.py so that it runs a sliding window along the input series and creates associated input/output pairs. Note that this function should input a) the series and b) the window length, and return the input/output subsequences. Make sure to format returned input/output as generally shown in table above (where window_size = 5), and make sure your returned input is a numpy array.\n",
    " \n",
    "**TODO** Es hora de escribir código. Implementa una función llamada window_transform_series. Esta función debe correr una ventana que se deslice por los pares de input/output. Esta función debe tomar como entrada una serie y el tamaño T de la ventana. A su vez, la función debe regresar una lista con los vectores de entrada y otra lista con los escalares de salida. Asegúrate que el formato sea como arriba y que el valor regresado sea un arreglo de numpy. No veas la solución directamente. \n",
    "\n",
    "Puedes probar tu función con las dos siguientes celdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_nums = np.array([1,3,5,7,9,11,13])\n",
    "# Esperaríamos \n",
    "# X = [[1 3] [3 5] ... [9 11]]\n",
    "# y = [5 7 ... 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corremos una ventana con tamaño 2\n",
    "window_size = 2\n",
    "X,y = window_transform_series(odd_nums, window_size)\n",
    "\n",
    "print ('--- the input X will look like ----')\n",
    "print (X)\n",
    "\n",
    "print ('--- the associated output y will look like ----')\n",
    "print (y)\n",
    "\n",
    "print ('the shape of X is ' + str(np.shape(X)))\n",
    "print ('the shape of y is ' + str(np.shape(y)))\n",
    "print('the type of X is ' + str(type(X)))\n",
    "print('the type of y is ' + str(type(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: llena la siguiente función.\n",
    "def window_transform_series(series, window_size):\n",
    "    \"\"\"\n",
    "    Implementa la función para la ventana que se desliza. Toma tu tiempo para este problema\n",
    "    y no veas la solución inmediatamente. Piensa en el vector de ejemplo. La función debería\n",
    "    ser entre 10 y 30 líneas, pero asegúrate de escribir un algoritmo que entiendas.\n",
    "    \n",
    "    series: Una lista con todos los elementos.\n",
    "    window_size: El tamaño de la ventana\n",
    "    \n",
    "    Tips: \n",
    "        Piensa cómo recorrer P-T elementos y cómo ir agregando los elementos a la lista.\n",
    "        Al final convertí los resultados a np array y convertí y a un vector columna\n",
    "    \"\"\"\n",
    "    # containers for input/output pairs\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    y.shape = (len(y),1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 7\n",
    "X,y = window_transform_series(series = dataset, window_size = window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Separar en training y testing set\n",
    "\n",
    "Para mantenerlo sencillo, usaremos 33% del dataset para testing. Cuando el modelo esté entrenado, veremos su precisión en el testing set. Normalmente, separaríamos la información de manera aleatoria para un modelo de regresión. Aquí no haremos esto. ¿Por qué? En este caso, nuestros pares están relacionados temporalmente. No podemos validar nuestro modelo entrenándolo en un conjunto aleatorio de la serie y luego probándolo en otro. Queremos entrenar el modelo en un periodo de la serie y validarlo en otro periodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separamos en training y testing\n",
    "train_test_split = int(np.ceil(2*len(y)/float(3)))   # split point\n",
    "\n",
    "# obtenemos train set\n",
    "X_train = X[:train_test_split,:]\n",
    "y_train = y[:train_test_split]\n",
    "\n",
    "# obtenemos test set\n",
    "X_test = X[train_test_split:,:]\n",
    "y_test = y[train_test_split:]\n",
    "\n",
    "# NOTE: Para usar LSTM, nuestro input debe ser modificado a [samples, window_size, stepsize]\n",
    "X_train = np.asarray(np.reshape(X_train, (X_train.shape[0], window_size, 1)))\n",
    "X_test = np.asarray(np.reshape(X_test, (X_test.shape[0], window_size, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Contruir la red neuronal con RNN\n",
    "\n",
    "Ya teniendo nuestros pares de la serie con la función que implementaste y con los conjuntos de training y testing, podemos crear la red neuronal. Utilizaremos Keras para construir una pequeña red neuronal de do scapas:\n",
    "* Capa 1: Usa un módulo LSTM con 5 hidden units. Nota que el input_shape=(window_size, 1)\n",
    "* Capa 2: Usa una capa completamente conectada con una unidad\n",
    "\n",
    "Revisa\n",
    "- https://keras.io/layers/recurrent/\n",
    "- https://keras.io/optimizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: create required RNN model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import keras\n",
    "\n",
    "# TODO: construye una RNN para hacer regresión en nuestra serie de tiempo con entrada/salida. Son 3 líneas de código.\n",
    "# Recuerda usar LSTM, no RNN. Sólo son dos parámetros - unidades e input_shape\n",
    "\n",
    "# podemos configurar el optimizador por separado para especificar todos sus argumentos\n",
    "# https://keras.io/optimizers/ Revisa sus parámetros\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run your model!\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=50, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Revisamos Desempeño del modelo\n",
    "Si todo funciona bien, se espera un error menor a 0.02. Si cualquiera de los dos errores es menor, cambia el número de epochs (1000 debería ser suficiente) o cambia el batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el modelo para training y testing.\n",
    "training_error = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('training error = ' + str(training_error))\n",
    "\n",
    "testing_error = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('testing error = ' + str(testing_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot everything - the original series as well as predictions on training and testing sets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# predict in x and y\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "# plot original series\n",
    "plt.plot(dataset,color = 'k')\n",
    "\n",
    "# plot training set prediction\n",
    "split_pt = train_test_split + window_size \n",
    "plt.plot(np.arange(window_size,split_pt,1),train_predict,color = 'b')\n",
    "\n",
    "# plot testing set prediction\n",
    "plt.plot(np.arange(split_pt,split_pt + len(test_predict),1),test_predict,color = 'r')\n",
    "\n",
    "# pretty up graph\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('(normalized) price of Apple stock')\n",
    "plt.legend(['original series','training fit','testing fit'],loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2. Generador de Secuencias\n",
    "\n",
    "### 2.1 Inicio\n",
    "En esta parte implementaras una RNN que escriba secuencias de inglés que escriba oraciones relativamente coherentes desde nada. Este modelo construirá las secuencias caracter por caracter. Esto requerirá ajustar muchos parámetros en el cuerpo de palabras de entrenamiento. Utilizaremos la versión completa de las aventuras de Sherlock Holmes para entrenar.\n",
    "\n",
    "Aunque es una tarea completamente diferente, el proceso es muy similar. Podemos enseñar al modelo a aprender patrones de entradas y salidas a partir de muchos ejemplos. Cada entrada es un string como el siguiente:\n",
    "\n",
    "dogs are grea\n",
    "\n",
    "mientras que su salida correspondiente es el siguiente caracter de la secuencia ('t'). Para que las predicciones sean razonables, el modelo debe ver muchos ejemplos. El programa funcionará más o menos así:\n",
    "\n",
    "* http://www.cs.toronto.edu/~ilya/rnn.html\n",
    "\n",
    "Esto es usado por bots en twitter y hay un concurso anual de modelos que generan novelas de más de 50,000 palabras (https://github.com/NaNoGenMo/2016).\n",
    "\n",
    "\n",
    "### 2.2 Preprocesamos el dataset\n",
    "\n",
    "La primera tarea es cargar un corpus para usar en el entrenamiento y hacer una serie de tareas. Podrías usar cualquier cuerpo de 100,000 caracteres o más, pero aquí usaremos el libro de Sherlock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leemos y convertimos a minúsculas\n",
    "text = open('data/holmes.txt').read().lower()\n",
    "print('our original text has ' + str(len(text)) + ' characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a examinar el texto. Sólo nos interesa entrenar a partir de palabras válidas en inglés. Es decir, podemos eliminar cualquier caracter que no sea parte de palabras en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente hay mucha basura que no nos sirve y podemos quitar. Como los primeros caracteres contienen la tabla de contenido, el título y otras cosas que no nos importan, quitaremos los primeros caracteres. También remplazaremos los \\n con espacios vacíos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[1302:]\n",
    "text = text.replace('\\n',' ')    # replacing '\\n' with '' simply removes the sequence\n",
    "text = text.replace('\\r',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sólo nos quedaremos con letras y algunos símbolos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "allowed_chars = string.ascii_lowercase + ' ' + '!' + ',' + '.' + ':' + ';' + '?'\n",
    "\n",
    "# remove as many non-english characters and character sequences as you can \n",
    "for char in text:\n",
    "    if char not in allowed_chars:\n",
    "        text = text.replace(char, ' ')\n",
    "\n",
    "# shorten any extra dead space created above\n",
    "text = text.replace('  ',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# print some of the text, as well as statistics\n",
    "print (\"this corpus has \" +  str(len(text)) + \" total number of characters\")\n",
    "print (\"this corpus has \" +  str(len(chars)) + \" unique characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Separamos data en pares\n",
    "\n",
    "Al igual que antes, vamos a crear un conjunto de pares de entrada y salida para que el modelo pueda entrenarse. En la primera parte implementaste una ventana que se deslica en una serie de tiempo y extrae los pares. Aquí haremos lo mismo. Tendremos una ventana dde tamaño $T$ que se deslizará por el enorme corpus. Lo que está en la ventana es el input y el siguiente caracter es su output correspondiente. Aquí hay un ejemplo de T = 5.\n",
    "\n",
    "![imagen](https://github.com/udacity/aind2-rnn/blob/master/images/text_windowing.gif?raw=true)\n",
    "\n",
    "Nota que aquí, aunque tenemos una ventana, no es exactamente igual - la ventana no se desliza un caracter a la vez, sino que se mueve en un step definido $M$. Esto se hace en entradas largas de texto porque sino habría demasiados pares de entradas y salidas. \n",
    "\n",
    "Podemos definir el corpus como:\n",
    "\n",
    "$$s_{0},s_{1},s_{2},...,s_{P}$$\n",
    "donde $P$ es el tamaño del texto (aprox 500,000). Tenemos una ventana de tamaño T = 5 y con un paso de M = 1. Eso produce los siguientes pares\n",
    "\n",
    "$$\\begin{array}{c|c}\n",
    "\\text{Input} &amp; \\text{Output}\\\\\n",
    "\\hline \\color{CornflowerBlue} {\\langle s_{1},s_{2},s_{3},s_{4},s_{5}\\rangle} &amp; \\color{Goldenrod}{ s_{6}} \\\\\n",
    "\\ \\color{CornflowerBlue} {\\langle s_{2},s_{3},s_{4},s_{5},s_{6} \\rangle } &amp; \\color{Goldenrod} {s_{7} } \\\\\n",
    "\\color{CornflowerBlue}  {\\vdots} &amp; \\color{Goldenrod} {\\vdots}\\\\\n",
    "\\color{CornflowerBlue} { \\langle s_{P-5},s_{P-4},s_{P-3},s_{P-2},s_{P-1} \\rangle } &amp; \\color{Goldenrod} {s_{P}}\n",
    "\\end{array}$$\n",
    "\n",
    "Mira que cada entrada es una secuencia de 5 caracteres y que el output es un caracter. Normalmente, dada una M, creamos ceil(P/M) pares aproximadamente.\n",
    "\n",
    "**TODO**: Crea una función similar a la de la parte 1. En esta función, se debe correr una ventana y generar sus pares asociados. Esta función recibe el texto, el tamaño de la ventana y el step size. En este caso, el valor que regresan deben ser listas, no arreglos de numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: fill out the function below that transforms the input text and window-size into a set of input/output pairs for use with our RNN model\n",
    "def window_transform_text(text,window_size,step_size):\n",
    "    # containers for input/output pairs\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    ctr = 0\n",
    "    \n",
    "    # TODO: Revisa los parámetros de range de python. Se puede hacer en un for con 3 líneas.\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "step_size = 5\n",
    "inputs, outputs = window_transform_text(text,window_size,step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out a few of the input/output pairs to verify that we've made the right kind of stuff to learn from\n",
    "print('input = ' + inputs[2])\n",
    "print('output = ' + outputs[2])\n",
    "print('--------------')\n",
    "print('input = ' + inputs[100])\n",
    "print('output = ' + outputs[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Revisión del problema\n",
    "\n",
    "El problema de predicción de series de tiempo era un problema de regresión porque el output era una valor continuo. Aquí, generamos caracter por caracter. Este es un valor discreto, por lo que estamos hablando de un problema de clasificación. ¿Cuántas clases tenemos? El número de caracteres únicos que podemos predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print (\"this corpus has \" +  str(len(chars)) + \" unique characters\")\n",
    "print ('and these characters are ')\n",
    "print (chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El único problema aquí es que necesitamos datos numéricos, por lo que debemos transformar los caracteres. La manera más sencilla es con one-hot encoding como lo hemos hecho en otros problemas. \n",
    "\n",
    "\n",
    "$$a\\longleftarrow\\left[\\begin{array}{c}\n",
    "1\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "0\n",
    "\\end{array}\\right]\\,\\,\\,\\,\\,\\,\\,b\\longleftarrow\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "0\n",
    "\\end{array}\\right]\\,\\,\\,\\,\\,c\\longleftarrow\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "1\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "0 \n",
    "\\end{array}\\right]\\cdots$$\n",
    "\n",
    "cada vector tendrá 32 entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dictionary is a function mapping each unique character to a unique integer\n",
    "chars_to_indices = dict((c, i) for i, c in enumerate(chars))  # map each unique character to unique integer\n",
    "\n",
    "# this dictionary is a function mapping each unique integer back to a unique character\n",
    "indices_to_chars = dict((i, c) for i, c in enumerate(chars))  # map each unique integer back to unique character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform character-based input/output into equivalent numerical versions\n",
    "def encode_io_pairs(text,window_size,step_size):\n",
    "    # number of unique chars\n",
    "    chars = sorted(list(set(text)))\n",
    "    num_chars = len(chars)\n",
    "    \n",
    "    # cut up text into character input/output pairs\n",
    "    inputs, outputs = window_transform_text(text,window_size,step_size)\n",
    "    \n",
    "    # create empty vessels for one-hot encoded input/output\n",
    "    X = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n",
    "    y = np.zeros((len(inputs), num_chars), dtype=np.bool)\n",
    "    \n",
    "    # loop over inputs/outputs and tranform and store in X/y\n",
    "    for i, sentence in enumerate(inputs):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, chars_to_indices[char]] = 1\n",
    "        y[i, chars_to_indices[outputs[i]]] = 1\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "step_size = 5\n",
    "X, y = encode_io_pairs(text, window_size, step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Configurando nuestra RNN\n",
    "\n",
    "Ahora, al igual que antes, construiremos nuestra RNN. Esta vez, la red neuronal será:\n",
    "* Capa 1: Un módulo LSTM con 200 unidades ocultas. Nota que input_shape=(window_size,len(unique_chars))\n",
    "* Capa 2: Capa completamente conectada con len(unique_chars) unidades ocultas (es la capa de salida) activada con softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### necessary functions from the keras library\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras\n",
    "import random\n",
    "\n",
    "# TODO build the required RNN model: a single LSTM hidden layer with softmax activation, categorical_crossentropy loss \n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# compile model --> make sure initialized optimizer and callbacks - as defined above - are used\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small subset of our input/output pairs\n",
    "Xsmall = X[:10000,:,:]\n",
    "ysmall = y[:10000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nota que normalmente haríamos 50 a 100 iteraciones\n",
    "model.fit(Xsmall, ysmall, batch_size=500, epochs=30,verbose = 1)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('best_RNN_small_textdata_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo podemos predecir a partir del modelo?\n",
    "\n",
    "Primero, predecimos el siguiente caracter dado un conjunto de caracteres cuyo tamaño es el mismo que el de la ventana. Luego, quitamos el primer caracter de la entrada y agregamos la predicción. Esta es nuestra nueva secuencia que podemos usar como entrada. Repetimos este proceso N veces para conseguir N predicciones. \n",
    "\n",
    "En la siguiente celda hay una función que hace eso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that uses trained model to predict a desired number of future characters\n",
    "def predict_next_chars(model, input_chars, num_to_predict):     \n",
    "    predicted_chars = ''\n",
    "    \n",
    "    for i in range(num_to_predict):\n",
    "        # convierte entrada a números  \n",
    "        x_test = np.zeros((1, window_size, len(chars)))\n",
    "        for t, char in enumerate(input_chars):\n",
    "            x_test[0, t, chars_to_indices[char]] = 1.\n",
    "\n",
    "        # predice en esta iteración\n",
    "        test_predict = model.predict(x_test,verbose = 0)[0]\n",
    "\n",
    "        # convierte predicción a caracter\n",
    "        r = np.argmax(test_predict)                           # predict class of each test input\n",
    "        d = indices_to_chars[r] \n",
    "\n",
    "        # agrega nuevo caracter y quita el primero\n",
    "        predicted_chars+=d\n",
    "        input_chars+=d\n",
    "        input_chars = input_chars[1:]\n",
    "    return predicted_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_inds = [0, 500, 1000]\n",
    "\n",
    "# cargamos pesos\n",
    "model.load_weights('best_RNN_small_textdata_weights.hdf5')\n",
    "\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = text[start_index: start_index + window_size]\n",
    "\n",
    "    # usa la función que utilizamos\n",
    "    predict_input = predict_next_chars(model, input_chars, num_to_predict = 100)\n",
    "\n",
    "    # print out input characters\n",
    "    print('------------------')\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "    print(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ve aceptable, pero no tiene mucho sentido. Intentemos con un conjunto más grande de datos. Configurar los parámetros de un RNN para un problema grande como este normalmente requiere un GPU para acelerar el proceso por 10. Como el entrenamiento tarda mucho, se sugiere que escribas cada output de cada paso a un archivo. De esta manera, aunque ya no tengas el cuaderno abierto, podrás ver los resultados en el procesamiento. Esta configuración se hace así."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A simple way to write output to file\n",
    "f = open('my_test_output.txt', 'w')              # create an output file to write too\n",
    "f.write('this is only a test ' + '\\n')           # print some output text\n",
    "x = 2\n",
    "f.write('the value of x is ' + str(x) + '\\n')    # record a variable value\n",
    "f.close()     \n",
    "\n",
    "# print out the contents of my_test_output.txt\n",
    "f = open('my_test_output.txt', 'r')              # create an output file to write too\n",
    "f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No corras esta celda\n",
    "# Aquí utilizamos más caracteres. Aquí cada iteración se tardará 10-20 minutos, por lo que no te sugiero apenas que\n",
    "# quieras dejarlo en la noche corriendo.\n",
    "Xlarge = X[:100000,:,:]\n",
    "ylarge = y[:100000,:]\n",
    "\n",
    "# TODO: fit to our larger dataset\n",
    "model.fit(Xlarge, ylarge, batch_size=500, nb_epoch=30,verbose = 1)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('best_RNN_large_textdata_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_inds = [0, 500, 1000]\n",
    "\n",
    "# save output\n",
    "f = open('RNN_large_textdata_output.txt', 'w')  # create an output file to write too\n",
    "\n",
    "# load weights\n",
    "model.load_weights('model_weights/best_RNN_large_textdata_weights.hdf5')\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = text[start_index: start_index + window_size]\n",
    "\n",
    "    # use the prediction function\n",
    "    predict_input = predict_next_chars(model,input_chars,num_to_predict = 100)\n",
    "\n",
    "    # print out input characters\n",
    "    line = '-------------------' + '\\n'\n",
    "    print(line)\n",
    "    f.write(line)\n",
    "\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "    print(input_line)\n",
    "    f.write(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    predict_line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(predict_line)\n",
    "    f.write(predict_line)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
